name: Adversarial Testing

# This workflow is Sentinel attacking itself
# Only runs on manual trigger or red-team branches
# Answers: "Would Sentinel catch these vulnerabilities if scanning itself?"

on:
  workflow_dispatch:
    inputs:
      attack_intensity:
        description: 'Attack intensity (low/medium/high)'
        required: true
        default: 'medium'
        type: choice
        options:
          - low
          - medium
          - high
  push:
    branches:
      - 'red-team/**'

jobs:
  self-scan:
    name: Sentinel Scans Itself
    runs-on: macos-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install bandit semgrep safety

      - name: Run self-scan simulation
        run: |
          echo "üéØ Sentinel attacking itself..."

          # Would Sentinel detect its own vulnerabilities?
          echo "=== Phase 1: Static Analysis ==="
          bandit -r core/ -ll || true

          echo ""
          echo "=== Phase 2: Pattern Detection ==="

          # Command injection detection
          if grep -r "shell=True" core/ --include="*.py"; then
            echo "‚úÖ FOUND: Command injection vulnerability (shell=True)"
          else
            echo "‚ùå MISSED: Command injection"
          fi

          # Unsafe subprocess
          if grep -r "subprocess.call" core/ --include="*.py"; then
            echo "‚úÖ FOUND: Unsafe subprocess usage"
          else
            echo "‚ùå MISSED: Unsafe subprocess"
          fi

          # AI code generation without validation
          if grep -r "compile_exploit\|generate.*code" core/ --include="*.py"; then
            echo "‚úÖ FOUND: AI code generation (potential injection)"
          else
            echo "‚ùå MISSED: AI code generation"
          fi

          # MITM capability
          if grep -r "mitmproxy\|HTTPFlow" core/ --include="*.py"; then
            echo "‚úÖ FOUND: MITM capability (ghost/lazarus)"
          else
            echo "‚ùå MISSED: MITM capability"
          fi

          echo ""
          echo "=== Phase 3: Behavioral Analysis ==="

          # Check if autonomous exploitation is detected
          if grep -r "auto.*exploit\|Hand of God" core/ --include="*.py"; then
            echo "‚úÖ FOUND: Autonomous exploitation capability"
          else
            echo "‚ùå MISSED: Autonomous exploitation"
          fi

      - name: Generate attack surface map
        run: |
          echo "üó∫Ô∏è  Generating attack surface map..."

          python -c "
          import sys
          from pathlib import Path
          import json

          sys.path.insert(0, 'core')

          attack_surface = {
              'command_execution': [],
              'network_exposure': [],
              'ai_processing': [],
              'autonomous_actions': [],
              'data_storage': []
          }

          # Find all attack vectors
          for py_file in Path('core').rglob('*.py'):
              content = py_file.read_text()

              if 'subprocess' in content or 'shell=True' in content:
                  attack_surface['command_execution'].append(str(py_file))

              if '0.0.0.0' in content or 'bind(' in content:
                  attack_surface['network_exposure'].append(str(py_file))

              if 'ollama' in content.lower() or 'ai' in content.lower():
                  attack_surface['ai_processing'].append(str(py_file))

              if 'auto' in content and 'exploit' in content:
                  attack_surface['autonomous_actions'].append(str(py_file))

              if 'sqlite' in content or 'database' in content:
                  attack_surface['data_storage'].append(str(py_file))

          # Save report
          with open('attack-surface.json', 'w') as f:
              json.dump(attack_surface, f, indent=2)

          print(json.dumps(attack_surface, indent=2))

          # Count attack vectors
          total_vectors = sum(len(v) for v in attack_surface.values())
          print(f'\nüìä Total attack vectors: {total_vectors}')
          "

      - name: Upload attack surface report
        uses: actions/upload-artifact@v4
        with:
          name: attack-surface-map
          path: attack-surface.json
          retention-days: 90

  fuzzing-simulation:
    name: Input Fuzzing Tests
    runs-on: macos-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest hypothesis

      - name: Fuzz API inputs
        run: |
          echo "üé≤ Fuzzing API input validation..."

          python -c "
          import sys
          sys.path.insert(0, 'core')

          from server.api import ScanRequest

          # Test payloads that should be rejected
          dangerous_inputs = [
              '; rm -rf /',
              '&& cat /etc/passwd',
              '\$(whoami)',
              '../../../etc/passwd',
              '<script>alert(1)</script>',
              '{{7*7}}',
              '\x00\x01\x02',
              'a' * 10000,
          ]

          blocked = 0
          for payload in dangerous_inputs:
              try:
                  req = ScanRequest(target=payload)
                  print(f'‚ùå FAILED TO BLOCK: {repr(payload)}')
              except ValueError:
                  blocked += 1

          print(f'\n‚úÖ Blocked {blocked}/{len(dangerous_inputs)} dangerous inputs')

          if blocked < len(dangerous_inputs) * 0.8:
              print('‚ö†Ô∏è  Warning: Input validation may be insufficient')
          "

      - name: Fuzz tool argument injection
        run: |
          echo "üéØ Testing tool argument injection..."

          python -c "
          import sys
          sys.path.insert(0, 'core')

          # Simulate tool argument substitution
          test_cases = [
              ('nmap {target}', '; whoami', 'Command injection'),
              ('curl {target}', '| bash', 'Pipe injection'),
              ('python {target}', '../../../secret.py', 'Path traversal'),
          ]

          for template, malicious_input, attack_type in test_cases:
              result = template.replace('{target}', malicious_input)
              if ';' in result or '|' in result or '..' in result:
                  print(f'‚ö†Ô∏è  {attack_type}: {result}')

          print('‚úÖ Fuzzing complete - review results above')
          "

  exploit-verification:
    name: Exploit Generation Safety Test
    runs-on: macos-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Test AI-generated code validation
        run: |
          echo "üß™ Testing exploit compiler safety..."

          python -c "
          import sys
          import ast
          sys.path.insert(0, 'core')

          # Sample AI-generated exploit (potentially malicious)
          test_exploits = [
              # Safe exploit
              '''
          import requests
          response = requests.get('http://example.com/test')
          print(response.status_code)
          ''',
              # Malicious exploit (should be caught)
              '''
          import os
          os.system('rm -rf /')
          ''',
              # Sneaky malicious
              '''
          import subprocess
          subprocess.run(['curl', 'http://attacker.com'], shell=True)
          ''',
          ]

          print('Testing AST validation...')

          for i, exploit_code in enumerate(test_exploits):
              try:
                  tree = ast.parse(exploit_code)

                  # Look for dangerous patterns
                  dangerous = False
                  for node in ast.walk(tree):
                      # Check for os.system
                      if isinstance(node, ast.Call):
                          if isinstance(node.func, ast.Attribute):
                              if node.func.attr in ['system', 'popen', 'spawn']:
                                  print(f'Exploit {i+1}: ‚ùå DANGEROUS - os.system/popen/spawn')
                                  dangerous = True

                      # Check for shell=True
                      if isinstance(node, ast.keyword):
                          if node.arg == 'shell' and isinstance(node.value, ast.Constant):
                              if node.value.value is True:
                                  print(f'Exploit {i+1}: ‚ùå DANGEROUS - shell=True')
                                  dangerous = True

                  if not dangerous:
                      print(f'Exploit {i+1}: ‚úÖ SAFE')

              except SyntaxError:
                  print(f'Exploit {i+1}: ‚ùå SYNTAX ERROR')

          print('\n‚ö†Ô∏è  NOTE: Sentinel does NOT currently validate AI-generated code!')
          print('This test demonstrates what SHOULD be implemented.')
          "

  privilege-escalation-test:
    name: Privilege Escalation Detection
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check for setuid/sudo usage
        run: |
          echo "üîì Checking for privilege escalation vectors..."

          # Check for setuid
          if find . -type f -perm -4000 2>/dev/null | grep -v ".git"; then
            echo "‚ùå CRITICAL: setuid files found"
            exit 1
          fi

          # Check for sudo in code
          if grep -r "sudo " core/ --include="*.py" --include="*.sh"; then
            echo "‚ö†Ô∏è  Warning: sudo usage found in code"
          fi

          # Check for capability manipulation
          if grep -r "CAP_\|setcap" core/ --include="*.py"; then
            echo "‚ö†Ô∏è  Warning: capability manipulation found"
          fi

          echo "‚úÖ No obvious privilege escalation vectors"

  secrets-leakage-test:
    name: Secrets Leakage Prevention
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install truffleHog
        run: |
          pip install truffleHog

      - name: Scan for secrets
        run: |
          echo "üîç Scanning for leaked secrets..."

          # Scan git history for secrets
          trufflehog filesystem . --json > secrets-report.json || true

          # Check if any secrets found
          if [ -s secrets-report.json ]; then
            echo "‚ö†Ô∏è  Potential secrets found:"
            cat secrets-report.json
            echo ""
            echo "Review these manually - may be false positives"
          else
            echo "‚úÖ No secrets detected in git history"
          fi

      - name: Check for API keys in code
        run: |
          echo "üîë Checking for hardcoded API keys..."

          # Common API key patterns
          if grep -r -E "(api[_-]?key|apikey|api[_-]?secret)\s*=\s*['\"][^'\"]{20,}" \
              core/ --include="*.py" --include="*.yml"; then
            echo "‚ùå Potential hardcoded API keys found"
            exit 1
          fi

          echo "‚úÖ No hardcoded API keys detected"

  docker-security-scan:
    name: Docker Image Security Scan
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Build Docker image
        run: |
          docker build -t sentinel-test:latest .

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'sentinel-test:latest'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Check Dockerfile best practices
        run: |
          echo "üê≥ Checking Dockerfile security..."

          # Check for non-root user
          if ! grep -q "USER " Dockerfile; then
            echo "‚ö†Ô∏è  Warning: Dockerfile runs as root (security risk)"
          fi

          # Check for EXPOSE 0.0.0.0
          if grep -q "0.0.0.0" Dockerfile; then
            echo "‚ö†Ô∏è  Warning: Binding to 0.0.0.0 (exposes to network)"
          fi

          # Check for latest tag usage
          if grep -q ":latest" Dockerfile; then
            echo "‚ö†Ô∏è  Warning: Using :latest tag (not reproducible)"
          fi

          echo "‚úÖ Dockerfile security check complete"

  adversarial-summary:
    name: Adversarial Testing Summary
    runs-on: ubuntu-latest
    needs: [self-scan, fuzzing-simulation, exploit-verification, privilege-escalation-test, secrets-leakage-test, docker-security-scan]
    if: always()

    steps:
      - name: Generate security report
        run: |
          echo "üéØ Adversarial Testing Report"
          echo "=============================="
          echo ""
          echo "Test Results:"
          echo "- Self-scan: ${{ needs.self-scan.result }}"
          echo "- Input fuzzing: ${{ needs.fuzzing-simulation.result }}"
          echo "- Exploit safety: ${{ needs.exploit-verification.result }}"
          echo "- Privilege escalation: ${{ needs.privilege-escalation-test.result }}"
          echo "- Secrets leakage: ${{ needs.secrets-leakage-test.result }}"
          echo "- Docker security: ${{ needs.docker-security-scan.result }}"
          echo ""
          echo "Attack Intensity: ${{ github.event.inputs.attack_intensity || 'medium' }}"
          echo ""
          echo "This workflow simulates Sentinel attacking itself."
          echo "Review all warnings and findings carefully."
