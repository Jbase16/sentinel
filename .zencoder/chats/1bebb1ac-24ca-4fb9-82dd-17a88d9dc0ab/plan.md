# Feature development workflow

---

## Workflow Steps

### [x] Step: Requirements

Your job is to generate a Product Requirements Document based on the feature description,

First, analyze the provided feature definition and determine unclear aspects. For unclear aspects: - Make informed guesses based on context and industry standards - Only mark with [NEEDS CLARIFICATION: specific question] if: - The choice significantly impacts feature scope or user experience - Multiple reasonable interpretations exist with different implications - No reasonable default exists - Prioritize clarifications by impact: scope > security/privacy > user experience > technical details

Ask up to 5 most priority clarifications to the user. Then, create the document following this template:

```
# Feature Specification: [FEATURE NAME]


## User Stories*


### User Story 1 - [Brief Title]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]
2. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

## Requirements*

## Success Criteria*

```

Save the PRD into `/Users/jason/Developer/sentinelforge/.zencoder/chats/1bebb1ac-24ca-4fb9-82dd-17a88d9dc0ab/requirements.md`.

### [x] Step: Technical Specification

Based on the PRD in `/Users/jason/Developer/sentinelforge/.zencoder/chats/1bebb1ac-24ca-4fb9-82dd-17a88d9dc0ab/requirements.md`, create a detailed technical specification to be used by a coding agent to implement the feature. Follow this template:

```
# Technical Specification: [FEATURE]

## Technical Context
Language/Version, primary dependencies, etc

## Technical Implementation Brief

Summarize key technical decisions for implementing the feature. Make sure they take into account the existing code as much as possible.

## Source Code Structure

## Contracts

Define addition or changes in data models, DB schemas, APIs, code interfaces etc

## Delivery Phases

Define several incremental deliverables for the feature. Each should be a minimal viable product testable end-to-end.

## Verification Strategy

Define how the coding agent can verify each deliverable it creates. Provide instructions for the agent to perform the verification using available tools (lint/test commands, bash commands) and create helper scripts and tools for more complex result verification.
The verification for each deliverable should be executable by a coding agent using built-in capabilities (lint and test commands from the project, bash commands), pre-generated helper scripts or MCP servers. Research and add to the spec:

- MCP servers that should be installed to help the agent with the verification

- helper scripts that need to be generated in the first phases of the plan to verify complex scenarios that can't be covered by the tests in the project's test framework(s)

- any sample input artifact(s) that are required for verification. Note if these artifacts can be a) generated by the agent; b) discovered by the agent on line; c) must be provided by the user.
```

Save the spec to `/Users/jason/Developer/sentinelforge/.zencoder/chats/1bebb1ac-24ca-4fb9-82dd-17a88d9dc0ab/spec.md`.

### [x] Step: Implementation Plan

Based on the technical spec in `/Users/jason/Developer/sentinelforge/.zencoder/chats/1bebb1ac-24ca-4fb9-82dd-17a88d9dc0ab/spec.md`, create a detailed task plan and update `/Users/jason/Developer/sentinelforge/.zencoder/chats/1bebb1ac-24ca-4fb9-82dd-17a88d9dc0ab/plan.md`. Each task should have task definition, references to contracts to be used/implemented, deliverable definition and verification instructions.

### [x] Step: Inventory and Evidence Harness
Implement `tools/ops/cleanup_disposition.py` inventory path to emit Evidence JSON (path, size_bytes, mtime, git_last_commit, git_deleted_previously, references, type) into `archive_stage/`; ensure type tagging covers code/doc/asset/build/test/model; verify by running `python -m tools.ops.cleanup_disposition --mode scan` and confirming inventory file exists with required fields.

### [x] Step: Disposition Engine and Reports
Add rules to produce DispositionRecord sets (delete|archive|keep|unknown) with confidence, rationale, questions; enforce invariants (delete >=0.9 confidence, no questions; archive only docs; partial holds uncertain); emit `deletion_list.json`, `partial_confidence.json`, `archive_manifest.json` in `archive_stage/`; verify invariants inside script and via inspection.

### [x] Step: Safety Simulation
Simulate deletion set against structure: run reference/import checks and `python tools/lint_structure.py` with deletion set excluded; if breakage detected, demote items to partial with rationale; verify simulation results recorded and lint passes.

### [x] Step: User Review Aids
Augment partial records with targeted questions and concise summaries; prepare optional Desktop archive manifest entries for doc candidates; verify questions present for all partial items and archive manifest populated when applicable.

### [x] Step: Final Validation
Run `python -m pytest tests` and `python tools/lint_structure.py` on current tree (no deletions applied) to ensure baseline health; summarize any failures tied to candidate deletions; ensure reports remain untouched during validation.
- pytest failed: python3.14 missing pytest module (not installed in env)
- lint_structure warnings: unexpected items at root (conflict.log, .DS_Store, core, .pytest_cache, ollama.log, .zenflow, .claude, models, TODO.md, .zencoder, sentinel.code-workspace, .gitignore, archive_stage, .venv, .github, venv, AGENTS.md, .git, .vscode, assets, server.log)
